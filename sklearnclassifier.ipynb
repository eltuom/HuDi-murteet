{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Projektissa murteiden luokittelua on kokeiltu alla olevalla scikit-learn-classifierilla\n",
        "- Luokitteluun sopivat txt-tiedostot löytyvät Altaasta, kunkin paikkakunnan korpus on varastoitu sellaisenaan yhdistelemättä muihin.\n",
        "- Tiedostoissa kunkin rivin muoto on: LABEL tab TEXT\n",
        "- Aiemmissa testeissä EP (Etelä-Pohjanmaa), PI (Pirkanmaa), SA (Satakunta),  VS (Varsinais-Suomi), KH (Kanta-Häme) ovat luokittelua varten jäsennelty LAN-labelilla.\n",
        "- Vastaavasti KA (Kainuu), ES (Etelä-Savo), PS (Pohjois-Savo), PK (Pohjois-Karjala), KS (Keski-Suomi) ovat jäsennelty ITA-labelilla.\n",
        "- Lisäksi paikkakunnat KP (Keski-Pohjanmaa), KY (Kymenlaakso), PH (Päijät-Häme), AH (Ahvenanmaa), EK (Etelä-Karjala), IU (Itä-Uusimaa), LA (Lappi), PO (Pohjanmaa), UU (Uusimaa).\n",
        "- Luokittelua on testattu myös murrepiirteistä johdetuilla merkkijonoilla, tässä tiedostossa liitettynä skripti muokkaamaan tiedosto edellä mainittuun muotoon (substringitjaljella.py).\n",
        " - Huom. tällaisenaan tarkastelussa vain kommentit, eli ei mahdollista huomioida annotaatioita. Seurauksena mukana on paljon roskaa.\n",
        " - Hakua on pyritty tarkentamaan luomalla training ja testing datat riveiltä, joista murrepiirteitä on löytynyt. Alustavasti vastaavaan seulontaan toimii Unix-komennotkin, mutta järkevää olisi luoda aineisto niin, että annotaatioiden avulla riveille saataisiin vain todelliset murrepiirteet.  \n"
      ],
      "metadata": {
        "id": "s5XEV1cH59hb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## sklearnsvm.py"
      ],
      "metadata": {
        "id": "tq9vk3by5-XA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Ho8PTBi5zq-"
      },
      "outputs": [],
      "source": [
        "import sklearn.feature_extraction\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "import sys\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "labels_train=[]\n",
        "labels_test=[]\n",
        "\n",
        "feats_train=[]\n",
        "feats_test=[]\n",
        "\n",
        "def read_file(file):\n",
        "    f_op = open(file,\"r\")\n",
        "    labels = []\n",
        "    feats = []\n",
        "    for line in f_op:\n",
        "        line=line.strip().split(\"\\t\")\n",
        "        if len(line) == 1:\n",
        "            continue\n",
        "        else:\n",
        "            feats.append(line[1])\n",
        "            labels.append(line[0])\n",
        "    return(labels,feats)\n",
        "\n",
        "labels_train,feats_train = read_file(sys.argv[1])\n",
        "labels_test,feats_test = read_file(sys.argv[2])\n",
        "\n",
        "def data_iterator(f):\n",
        "    for token in f:\n",
        "        yield token\n",
        "\n",
        "\n",
        "def tokenizer(txt):\n",
        "    \"\"\"Simple whitespace tokenizer\"\"\"\n",
        "    return txt.split()\n",
        "\n",
        "\n",
        "iterator=data_iterator(feats_train)\n",
        "test_iterator=data_iterator(feats_test)\n",
        "vectorizer=sklearn.feature_extraction.text.TfidfVectorizer(tokenizer=tokenizer,use_idf=True,min_df=0.0001)\n",
        "d=vectorizer.fit_transform(iterator)\n",
        "\n",
        "d_test=vectorizer.transform(test_iterator)\n",
        "\n",
        "classifier=SVC(C=0.7,kernel='linear',shrinking=False)\n",
        "classifier.fit(d,labels_train)\n",
        "labels_test_pred = classifier.predict(d_test)\n",
        "print(classification_report(labels_test, labels_test_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## substringitjaljella.py"
      ],
      "metadata": {
        "id": "W8Vz4RYj94Mg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "\n",
        "def process_feature(feature):\n",
        "    linguistic_features = [\n",
        "        ('elä', 3),\n",
        "        ('tt', 2),\n",
        "        ('ts', 2),\n",
        "        ('eir', 3),\n",
        "        ('yä', 2),\n",
        "        ('ht', 2),\n",
        "        ('eä', 2),\n",
        "        ('eiä', 3),\n",
        "        ('ua', 2),\n",
        "        ('ae',2),\n",
        "        ('hr',2)\n",
        "    ]\n",
        "    result = ''\n",
        "    i = 0\n",
        "    while i < len(feature):\n",
        "        found = False\n",
        "        for linguistic_feature, length in linguistic_features:\n",
        "            if i + len(linguistic_feature) <= len(feature) and feature[i:i + len(linguistic_feature)].lower() == linguistic_feature:\n",
        "                result += linguistic_feature\n",
        "                i += length\n",
        "                found = True\n",
        "                break\n",
        "        if not found:\n",
        "            result += ' '\n",
        "            i += 1\n",
        "    return result\n",
        "\n",
        "for line in sys.stdin:\n",
        "    if line.strip() != '':\n",
        "        label, feature = line.split('\\t', 1)\n",
        "        processed_feature = process_feature(feature)\n",
        "        print(f\"{label}\\t{processed_feature}\")"
      ],
      "metadata": {
        "id": "AS1sXktg93ZQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}